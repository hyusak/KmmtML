https://github.com/fire717/Machine-Learning/blob/master/other/note/FeatureEngneering.md

一. 数据分析
   1. 数据结构分析
   2. 画图matplotlib
   3. seaboan
二. 数据预处理
 基础方法

2.1 数据清洗
2.1.1 去重
2.1.2 过滤（把过高/过低的反常值用平滑值替代）

2.2缺失值处理
  LightGBM和XGBoost都能将NaN作为数据的一部分进行学习，所以不需要处理缺失值。
2.2.1 离散型一般用众数，连续型用中位数或者均值。
2.2.2 直接赋0/-1
2.2.3 根据经验直接赋值
2.2.4 用一个单独的机器学习算法通过其他特征来预测缺失值（回归算法）

2.3二值化
特征的二值化处理是将数值型数据输出为布尔类型。其核心在于设定一个阈值，当样本书籍大于该阈值时，输出为1，小于等于该阈值时输出为0。我们通常使用preproccessing库的Binarizer类对数据进行二值化处理。

2.4 标准化
比如身高和体重。SVM、神经网络、K-means之类要求数据标准化,把数据放缩到同样的范围 SVM/NN影响很大 树模型影响小。不是什么时候都需要标准化，比如物理意义非常明确的经纬度，如果标准化，其本身的意义就会丢失。
2.4.1 均值方差法
2.4.2 z-score标准化
2.4.3 StandardScaler标准化


2.5 归一化
2.5.1 最大最小归一化（最常用）
2.5.2 对数函数转换（log）
2.5.3 反余切转换

2.6区间缩放
2.6.1 sklearn.preprocessing.MaxAbsScaler - scikit-learn 0.18.1 documentation，将一列的数值，除以这一列的最大绝对值。不免疫outlier。
2.6.2 sklearn.preprocessing.MinMaxScaler - scikit-learn 0.18.1 documentation。不免疫outlier。

2.7 离散化
2.7.1 one-hot
2.7.2 连续数据离散-等宽离散，等频离散，聚类离散。  把数据按不同区间划分（等宽划分或等频划分），聚类编码/按层次进行编码
2.7.4 平均数编码（mean encoding）：针对高基数类别特征的有监督编码。当一个类别特征列包括了极多不同类别时（如家庭地址，动辄上万）时，可以采用。
2.7.5 低频类别：有时会有一些类别，在训练集和测试集中总共只出现一次，例如特别偏僻的郊区地址。此时，保留其原有的自然数编码意义不大，不如将所有频数为1的类别合并到同一个新的类别下。
2.7.6 hash编码成词向量

2.8 统计值
包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。

2.9 压缩范围
有些分类变量的少部分取值可能占据了90%的case，这种情况下可以采用预测模型、领域专家、或者简单的频率分布统计。具体问题具体分析，高频和低频都是需要特别处理的地方，抛弃效果不好时，可以考虑采样（高频）或上采样（低频），加权等等方法。

2.10 不平衡类别
2.10.1 集成学习+阈值调整
2.10.2 多的类别过采样/少的类别欠采样来平衡分布欠采样（undersampling）和过采样（oversampling）会对模型带来怎样的影响？

2.11 时间特征
2.11.1 提取成年/月/日
2.11.2 根据活动按周期提取（每月/周）
2.11.3 尝试模型ARMA/RNN（脸书开源工具FBprofit）。
2.11.4 这是一年的第n天，这是一年的第n周，这是一周的第n天，etc
2.11.5 时间序列：把昨天的特征加入今天的特征，或者把和昨天相比，特征数值的改变量加入今天的特征。

1.12 非正态分布转正态分布
非正态分布转正太分布（log），平方，立方，根号…（但任何针对单独特征列的单调变换（如对数）：不适用于决策树类算法。对于决策树而言，X 、X^3 、X^5 之间没有差异， |X| 、 X^2 、 X^4 之间没有差异，除非发生了舍入误差。）

三. 特征构造
3.1 特征提取
3.1.1 相关领域专家知识（比如速度与加速度，时域（均值方差等）与频域（傅立叶变换）等）
3.1.2 深度学习自动学习特征
3.1.3 原始数据本身就是特征
3.1.4 实验/经验/发现
3.1.5 特征组合：如对用户id和用户特征最组合来获得较大的特征集

3.2特征变换（普通加减乘除没有意义。）
3.2.1 不同阶的差分
3.2.2 傅立叶变换


3.3 特征组合
3.3.1多项式做组合特征（二次三次等）（sklearn.preprocessing.PolynomialFeatures - scikit-learn 0.18.1 documentation）对于树类模型没有多少意义
3.3.2 核方法
3.3.3 线性组合（linear combination）：仅适用于决策树以及基于决策树的ensemble（如gradient boosting, random forest），因为常见的axis-aligned split function不擅长捕获不同特征之间的相关性；不适用于SVM、线性回归、神经网络等。
3.3.4 比例特征（ratio feature）：X_1 / X_2
3.3.5 绝对值（absolute value）
3.3.6 max(X_1, X_2)，min(X_1, X_2)，X_1 xor X_2
3.3.7 类别特征与数值特征的组合：用N1和N2表示数值特征，用C1和C2表示类别特征，利用pandas的groupby操作，可以创造出以下几种有意义的新特征：（其中，C2还可以是离散化了的N1）
   median(N1)_by(C1) \ 中位数; mean(N1)_by(C1) \ 算术平均数; mode(N1)_by(C1) \ 众数;min(N1)_by(C1) \ 最小值; max(N1)_by(C1) \ 最大值; std(N1)_by(C1) \ 标准差 ; var(N1)_by(C1) \ 方差; freq(C2)_by(C1) \ 频数; freq(C1) \这个不需要groupby也有意义
   将这种方法和线性组合等基础特征工程方法结合（仅用于决策树），可以得到更多有意义的特征，如：N1 - median(N1)_by(C1); N1 - mean(N1)_by(C1)
3.3.8 用基因编程创造新特征 Welcome to gplearn’s documentation!
3.3.9 用决策树创造新特征:在决策树系列的算法中（单棵决策树、gbdt、随机森林），每一个样本都会被映射到决策树的一片叶子上。因此，我们可以把样本经过每一棵决策树映射后的index（自然数）或one-hot-vector（哑编码得到的稀疏矢量）作为一项新的特征，加入到模型中。 具体实现：apply()以及decision_path()方法，在scikit-learn和xgboost里都可以用。
3.3.10 Histogram映射：
3.3.11 特征交叉:

3.4 特征升维
3.4.1 核方法
3.4.2 autoencoder
3.4.3（CNN）多层神经网络编码

四. 特征选择
特征选择的的一般流程就是， 找一个集合，然后针对某个学习算法， 测试效果如何， 一直循环直到找到最优集合为止。但时间花费很大。

一般需要考虑两点：

特征是否发散：如果一个特征不发散，就是说这个特征大家都有或者非常相似，说明这个特征不需要。
特征和目标是否相关：与目标的相关性越高，越应该优先选择。
按照特征评价标准分类：

选择使分类器的错误概率最小的特征或者特征组合。
利用距离来度量样本之间相似度。
利用具有最小不确定性（Shannon熵、Renyi熵和条件熵）的那些特征来分类。
利用相关系数, 找出特征和类之间存在的相互关系；
利用特征之间的依赖关系, 来表示特征的冗余性加以去除。

4.1特征选择 特征选择， 经典三刀
4.1.1 过滤法Filter
4.1.2 封装法Wrapper
4.1.3 嵌入法Embedded(效果最好速度最快，模式单调，快速并且效果明显， 但是如何参数设置， 需要深厚的背景知识。)

4.2 特征降维
4.2.1 特征选择是在原本特征集合中取一部分出来，是特征集合的子集，特征降维做特征的计算组合后构成新特征。
4.2.2 线性降维
4.2.3 非线性降维（大多是流行学习）
4.2.4 迁移成分分析(TCA):不同领域之间迁移学习降维

人肉：SIFT, VLAD, HOG, GIST, LBP
模型：Sparse Coding, Auto Encoders, Restricted Boltzmann Machines, PCA, ICA, K-means